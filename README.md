# llama.rs

Rust bindings and tools for [llama.cpp](https://github.com/ggml-org/llama.cpp) - efficient LLM inference in Rust.

## Overview

This project provides Rust bindings and utilities for running large language models using the llama.cpp library, enabling fast and efficient inference on various hardware architectures.

## Getting Started

```bash
cargo build
cargo run --bin llama-cli
```

## Features

- [ ] Rust FFI bindings to llama.cpp
- [ ] High-level model loading API
- [ ] Inference session management
- [ ] Token streaming support
- [ ] Batch inference
- [ ] Memory-efficient inference

## License

MIT
