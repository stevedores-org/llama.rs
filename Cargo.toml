[package]
name = "llama-rs"
version = "0.1.0"
edition = "2021"
description = "High-performance LLM inference on Apple Silicon using Rust and MLX"
repository = "https://github.com/stevedores-org/llama.rs"
license = "MIT"

[dependencies]
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
memmap2 = "0.9"
byteorder = "1.5"
crossbeam-channel = "0.5"
half = { version = "2.4", features = ["serde"] }
thiserror = "2"

[dev-dependencies]
tempfile = "3.14"

[features]
default = []
metal = []

[[bin]]
name = "llama-cli"
path = "src/bin/cli.rs"

[lib]
name = "llama"
path = "src/lib.rs"
